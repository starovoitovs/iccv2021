{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from base_notebook.pose_data_tools.graph import Graph\n",
    "from base_notebook.pose_data_tools.generate_data import read_xyz\n",
    "from src.preprocessing.pre_normaliser import preNormaliser\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import iisignature\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AXES = 0, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(open(\"_input/train_data.npy\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('_input/train_label.csv')\n",
    "idx = labels[labels['label'].isin([0, 75])].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 100\n",
    "N_CLASSES = 100\n",
    "NOISE = 10.\n",
    "\n",
    "ps = np.zeros((0, 3, 1, 17, 2))\n",
    "\n",
    "for i in range(N_CLASSES):\n",
    "    p1 = data[100*i:100*i+1, :, :1, :, :]\n",
    "    zs = NOISE * np.random.standard_normal((N_SAMPLES, 3, 1, 17, 2)) + p1\n",
    "    ps = np.concatenate([ps, zs], axis=0)\n",
    "\n",
    "X = ps\n",
    "y = np.arange(N_CLASSES).repeat(N_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(N_SAMPLES*N_CLASSES)\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx].reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer, Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.ops import gen_math_ops\n",
    "from tensorflow.python.ops import special_math_ops\n",
    "\n",
    "graph = Graph()\n",
    "mat = graph.get_adjacency_matrix().astype(np.float32)\n",
    "\n",
    "class GraphAttention(Layer):\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        output_shape = 50\n",
    "        self.kernel = self.add_weight('kernel', (input_shape[3], input_shape[4], output_shape), trainable=True)\n",
    "        self.attention_mask = self.add_weight('attention_mask', mat[0].shape, trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        f0 = tf.einsum('ijklm,lmn->ikln', inputs, self.kernel)\n",
    "        att = gen_math_ops.Mul(x=mat[0]+mat[1]+mat[2], y=self.attention_mask)\n",
    "        att = tf.nn.softmax(att)\n",
    "        return tf.einsum('ml,ikln->ikn', att, f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x144290df0>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mat[1] + mat[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "2D convolution.\n",
       "\n",
       "Args:\n",
       "    x: Tensor or variable.\n",
       "    kernel: kernel tensor.\n",
       "    strides: strides tuple.\n",
       "    padding: string, `\"same\"` or `\"valid\"`.\n",
       "    data_format: `\"channels_last\"` or `\"channels_first\"`.\n",
       "    dilation_rate: tuple of 2 integers.\n",
       "\n",
       "Returns:\n",
       "    A tensor, result of 2D convolution.\n",
       "\n",
       "Raises:\n",
       "    ValueError: if `data_format` is neither `channels_last` or\n",
       "    `channels_first`.\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.9/site-packages/keras/backend.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "K.conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 10, 10, 3])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = tf.zeros([10,10,3])\n",
    "tf.expand_dims(image, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer, Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "\n",
    "graph = Graph()\n",
    "mat = graph.get_adjacency_matrix().astype(np.float32)\n",
    "\n",
    "class GraphAttention(Layer):\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        hidden_units = 20\n",
    "        self.kernel = self.add_weight('kernel', (input_shape[3], input_shape[4], hidden_units), trainable=True)\n",
    "        self.attention_mask = self.add_weight('attention_mask', mat[0].shape, trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        f0 = tf.einsum('ijklm,lmn->ikln', inputs, self.kernel)\n",
    "        att = tf.einsum('ij,jk', mat[0]+mat[1]+mat[2], self.attention_mask)\n",
    "        att = tf.nn.softmax(att)\n",
    "        return tf.einsum('ml,ijklo,lon->ikn', att, inputs, self.kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_44 (InputLayer)        [(None, 3, 601, 17, 2)]   0         \n",
      "_________________________________________________________________\n",
      "graph_attention_55 (GraphAtt (None, 601, 20)           969       \n",
      "_________________________________________________________________\n",
      "lambda_41 (Lambda)           (None, 4, 210)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 4, 210)            840       \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, 4, 64)             70400     \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 155)               39835     \n",
      "=================================================================\n",
      "Total params: 112,044\n",
      "Trainable params: 111,624\n",
      "Non-trainable params: 420\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_61 (InputLayer)        [(None, 3, 1, 17, 2)]     0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 3, 1, 17, 100)     300       \n",
      "_________________________________________________________________\n",
      "lambda_57 (Lambda)           (None, 4, 210)            0         \n",
      "_________________________________________________________________\n",
      "flatten_59 (Flatten)         (None, 840)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 155)               130355    \n",
      "=================================================================\n",
      "Total params: 130,655\n",
      "Trainable params: 130,655\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsiglen = iisignature.logsiglength(20, 2)\n",
    "\n",
    "N_TIMESTEPS = 100\n",
    "N_SEGMENTS = 24\n",
    "N_HIDDEN_GCN = 20\n",
    "\n",
    "input_layer = Input(shape=(3, N_TIMESTEPS, 17, 2))\n",
    "reshape_layer = Reshape()(input_layer)\n",
    "gcn_layer = Conv1D(20, 1)(reshape_layer)\n",
    "hidden_layer = Lambda(CLF, arguments=dict(number_of_segment=N_SEGMENTS, deg_of_logsig=2, logsiglen=logsiglen), output_shape=(N_SEGMENTS, logsiglen))(gcn_layer)\n",
    "BN_layer = BatchNormalization()(hidden_layer)\n",
    "flatten_layer = Flatten()(BN_layer)\n",
    "output_layer = Dense(155, activation='softmax')(flatten_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X[:, :, :N_SAMPLES, :, :], to_categorical(y), epochs=20, verbose=2, validation_split=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-15eaaba70a0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# cce = CategoricalCrossentropy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras import Model, Sequential\n",
    "from keras.layers import Flatten, BatchNormalization, Reshape, Dropout, Lambda, LSTM, Input, Conv2D, Conv1D, concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from src.algos.logsigrnn.sigutils import *\n",
    "from src.algos.logsigrnn.dyadic_sigutils import *\n",
    "import tensorflow as tf\n",
    "from base_notebook.pose_data_tools.graph import Graph\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "graph = Graph()\n",
    "mat = graph.get_adjacency_matrix().astype(np.float32)\n",
    "\n",
    "class GraphAttention(Layer):\n",
    "    \n",
    "    def __init__(self, n_hidden_units ,**kwargs):\n",
    "        super(GraphAttention, self).__init__(**kwargs)\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight('kernel', (input_shape[3], input_shape[4], self.n_hidden_units), trainable=True)\n",
    "        self.attention_mask = self.add_weight('attention_mask', mat[0].shape, trainable=True)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        f0 = tf.einsum('ijklm,lmn->ikln', inputs, self.kernel)\n",
    "        att = tf.einsum('ij,ij->ij', mat[0]+mat[1]+mat[2], self.attention_mask)\n",
    "        att = tf.nn.softmax(att)\n",
    "        return tf.einsum('ml,ijklo,lon->ikn', att, inputs, self.kernel)\n",
    "\n",
    "\n",
    "X = data[:, :, :, :, :]\n",
    "y = labels['label']\n",
    "\n",
    "N_SEGMENTS = 4\n",
    "N_HIDDEN_UNITS = 20\n",
    "\n",
    "logsiglen = iisignature.logsiglength(N_HIDDEN_UNITS, 2)\n",
    "\n",
    "input_layer = Input(shape=X.shape[1:])\n",
    "gcn_layer = GraphAttention(N_HIDDEN_UNITS)(input_layer)\n",
    "\n",
    "mid_output = Lambda(SP, arguments=dict(no_of_segments=N_SEGMENTS), output_shape=(N_SEGMENTS, N_HIDDEN_UNITS))(gcn_layer)\n",
    "hidden_layer = Lambda(CLF, arguments=dict(number_of_segment=N_SEGMENTS, deg_of_logsig=2, logsiglen=logsiglen), output_shape=(N_SEGMENTS, logsiglen))(gcn_layer)\n",
    "hidden_layer = Reshape((N_SEGMENTS, logsiglen))(hidden_layer)\n",
    "BN_layer = BatchNormalization()(hidden_layer)\n",
    "\n",
    "mid_input = concatenate([mid_output, BN_layer])\n",
    "lstm_layer = LSTM(units=64, return_sequences=True)(mid_input)\n",
    "drop_layer = Dropout(0.8)(lstm_layer)\n",
    "upper_mid_input = Flatten()(drop_layer)\n",
    "output_layer = Dense(155, activation='softmax')(upper_mid_input)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, to_categorical(y), epochs=20, verbose=2, validation_split=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(data):\n",
    "\n",
    "    N, C, T, V, M = data.shape\n",
    "\n",
    "    x0, x1 = np.min(data[:, AXES[0], :, :, :]), np.max(data[:, AXES[0], :, :, :])\n",
    "    y0, y1 = np.min(data[:, AXES[1], :, :, :]), np.max(data[:, AXES[1], :, :, :])\n",
    "\n",
    "    ratio = (y1 - y0) / (x1 - x0)\n",
    "\n",
    "    size = 3\n",
    "    \n",
    "    xh = size\n",
    "    yh = ratio * size\n",
    "\n",
    "    graph = Graph()\n",
    "    fig, ax = plt.subplots(figsize=(xh, yh))\n",
    "\n",
    "    plt.xlim((x0, x1))\n",
    "    plt.ylim((y0, y1))\n",
    "\n",
    "    edge = graph.inward\n",
    "\n",
    "    p_type = ['b-', 'g-', 'g-', 'c-', 'm-', 'y-', 'k-', 'k-', 'k-', 'k-']\n",
    "    pose = []\n",
    "\n",
    "    for m in range(M):\n",
    "        a = []\n",
    "        for i in range(len(edge)):\n",
    "            a.append(ax.plot(np.zeros(2), np.zeros(2), p_type[m])[0])\n",
    "        pose.append(a)\n",
    "\n",
    "    def animate(t):\n",
    "\n",
    "        for m in range(M):\n",
    "\n",
    "            for i, (v1, v2) in enumerate(edge):\n",
    "                x1 = data[0, AXES, t, v1, m]\n",
    "                x2 = data[0, AXES, t, v2, m]\n",
    "                if (x1.sum() != 0 and x2.sum() != 0) or v1 == 1 or v2 == 1:\n",
    "                    pose[m][i].set_xdata(data[0, AXES[0], t, [v1, v2], m])\n",
    "                    pose[m][i].set_ydata(data[0, AXES[1], t, [v1, v2], m])\n",
    "\n",
    "        return np.array(pose).flatten()\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=T, interval=20, blit=True)\n",
    "    return HTML(anim.to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
